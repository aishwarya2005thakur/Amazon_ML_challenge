{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwarya2005thakur/Amazon_ML_challenge/blob/main/Dibetes_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBvWPpJ-jsLU",
        "outputId": "98819a79-da19-45f1-9787-21ae993fccb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install pandas numpy scikit-learn xgboost lightgbm tqdm matplotlib seaborn pillow requests torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCj1Fs2Qkiua",
        "outputId": "c639d38d-29ec-431b-ee38-322807c3d992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded: (75000, 4) (75000, 3)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Option 1: Direct CSV Paths (Recommended) ===\n",
        "# Replace with your actual file paths inside Drive\n",
        "train_path = \"/content/drive/MyDrive/dataset/train.csv\"\n",
        "test_path = \"/content/drive/MyDrive/dataset/test.csv\"\n",
        "\n",
        "# === Option 2: If you still have a ZIP file ===\n",
        "# Uncomment below lines if your dataset is zipped in Drive\n",
        "# import zipfile\n",
        "# zip_path = \"/content/drive/MyDrive/your_folder/dataset.zip\"\n",
        "# extract_dir = \"/content/unzipped_data\"\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_dir)\n",
        "#\n",
        "# # Find train/test files after unzipping\n",
        "# for root, _, files in os.walk(extract_dir):\n",
        "#     for f in files:\n",
        "#         if f == \"train.csv\":\n",
        "#             train_path = os.path.join(root, f)\n",
        "#         if f == \"test.csv\":\n",
        "#             test_path = os.path.join(root, f)\n",
        "\n",
        "assert os.path.exists(train_path), f\"train.csv not found at {train_path}\"\n",
        "assert os.path.exists(test_path), f\"test.csv not found at {test_path}\"\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "train['catalog_content'] = train['catalog_content'].fillna('')\n",
        "test['catalog_content'] = test['catalog_content'].fillna('')\n",
        "\n",
        "y = train['price'].values\n",
        "use_log = True\n",
        "\n",
        "print(\"Loaded:\", train.shape, test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn59-qmKlA_M",
        "outputId": "65f14e88-0b63-43a3-f211-87c490aa4f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebuilding TF-IDF (20k features)...\n",
            "TF-IDF shapes: (75000, 20000) (75000, 20000)\n"
          ]
        }
      ],
      "source": [
        "#feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "train['catalog_content'] = train['catalog_content'].fillna('')\n",
        "test['catalog_content'] = test['catalog_content'].fillna('')\n",
        "\n",
        "print(\"Rebuilding TF-IDF (20k features)...\")\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=5)\n",
        "X_all = pd.concat([train['catalog_content'], test['catalog_content']], axis=0)\n",
        "tfidf.fit(X_all)\n",
        "X_train = tfidf.transform(train['catalog_content'])\n",
        "X_test = tfidf.transform(test['catalog_content'])\n",
        "print(\"TF-IDF shapes:\", X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVQlEhsbPoo5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VpzkU9OlELL",
        "outputId": "e8d2b207-2f9d-414b-ea57-5e409bdf07ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Combined shape: (75000, 20002) (75000, 20002)\n"
          ]
        }
      ],
      "source": [
        "# -------- Step 3: Extract quantity and unit features --------\n",
        "import re\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "def extract_pack_qty(text):\n",
        "    text = str(text).lower()\n",
        "    patterns = [\n",
        "        r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*pcs?', r'(\\d+)\\s*count',\n",
        "        r'x\\s*(\\d+)', r'(\\d+)\\s*pieces?', r'(\\d+)\\s*ct\\b'\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            return float(m.group(1))\n",
        "    return 1.0\n",
        "\n",
        "def extract_standard_weight(text):\n",
        "    text = str(text).lower()\n",
        "    patterns = [\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*kg', 1000.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*g', 1.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*mg', 0.001),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*l', 1000.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*ml', 1.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*oz', 28.35),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*pound', 453.6),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*lb', 453.6),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*fl', 29.57)\n",
        "    ]\n",
        "    for p, mult in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            try:\n",
        "                val = float(m.group(1))\n",
        "                # sanity check: skip nonsense like 0 or extremely small\n",
        "                if val > 0:\n",
        "                    return val * mult\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return 1.0\n",
        "\n",
        "# Apply\n",
        "train['item_pack_qty'] = train['catalog_content'].apply(extract_pack_qty)\n",
        "test['item_pack_qty'] = test['catalog_content'].apply(extract_pack_qty)\n",
        "\n",
        "train['standard_weight_ml'] = train['catalog_content'].apply(extract_standard_weight)\n",
        "test['standard_weight_ml'] = test['catalog_content'].apply(extract_standard_weight)\n",
        "\n",
        "# Log-scale\n",
        "for c in ['item_pack_qty', 'standard_weight_ml']:\n",
        "    train[c] = np.log1p(train[c])\n",
        "    test[c] = np.log1p(test[c])\n",
        "\n",
        "# Combine numeric + TF-IDF\n",
        "num_feats_train = train[['item_pack_qty', 'standard_weight_ml']].values\n",
        "num_feats_test  = test[['item_pack_qty', 'standard_weight_ml']].values\n",
        "\n",
        "X_train_comb = hstack([X_train, num_feats_train])\n",
        "X_test_comb  = hstack([X_test, num_feats_test])\n",
        "\n",
        "print(\" Combined shape:\", X_train_comb.shape, X_test_comb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEGt3q-FmsgB",
        "outputId": "5e4145bc-7da5-4345-9ba7-1f060d25cf4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tuned LightGBM...\n",
            "\n",
            "---- Fold 1/5 ----\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[200]\tvalid_0's l1: 0.535655\n",
            "[400]\tvalid_0's l1: 0.518886\n",
            "[600]\tvalid_0's l1: 0.511804\n",
            "[800]\tvalid_0's l1: 0.508698\n",
            "[1000]\tvalid_0's l1: 0.506719\n",
            "[1200]\tvalid_0's l1: 0.505256\n",
            "Early stopping, best iteration is:\n",
            "[1342]\tvalid_0's l1: 0.504456\n",
            "\n",
            "---- Fold 2/5 ----\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[200]\tvalid_0's l1: 0.526064\n",
            "[400]\tvalid_0's l1: 0.510218\n",
            "[600]\tvalid_0's l1: 0.503887\n",
            "[800]\tvalid_0's l1: 0.500831\n",
            "[1000]\tvalid_0's l1: 0.499101\n",
            "[1200]\tvalid_0's l1: 0.498284\n",
            "Early stopping, best iteration is:\n",
            "[1300]\tvalid_0's l1: 0.497634\n",
            "\n",
            "---- Fold 3/5 ----\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[200]\tvalid_0's l1: 0.528051\n",
            "[400]\tvalid_0's l1: 0.512765\n",
            "[600]\tvalid_0's l1: 0.506492\n",
            "[800]\tvalid_0's l1: 0.503715\n",
            "[1000]\tvalid_0's l1: 0.502026\n",
            "[1200]\tvalid_0's l1: 0.501165\n",
            "Early stopping, best iteration is:\n",
            "[1182]\tvalid_0's l1: 0.501102\n",
            "\n",
            "---- Fold 4/5 ----\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[200]\tvalid_0's l1: 0.518566\n",
            "[400]\tvalid_0's l1: 0.502577\n",
            "[600]\tvalid_0's l1: 0.496482\n",
            "[800]\tvalid_0's l1: 0.494135\n",
            "[1000]\tvalid_0's l1: 0.492577\n",
            "[1200]\tvalid_0's l1: 0.491765\n",
            "Early stopping, best iteration is:\n",
            "[1166]\tvalid_0's l1: 0.4917\n",
            "\n",
            "---- Fold 5/5 ----\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[200]\tvalid_0's l1: 0.529835\n",
            "[400]\tvalid_0's l1: 0.514567\n",
            "[600]\tvalid_0's l1: 0.508382\n",
            "[800]\tvalid_0's l1: 0.505179\n",
            "[1000]\tvalid_0's l1: 0.503643\n",
            "[1200]\tvalid_0's l1: 0.502718\n",
            "Early stopping, best iteration is:\n",
            "[1344]\tvalid_0's l1: 0.501655\n",
            "\n",
            "Tuned LightGBM SMAPE: 50.4 %\n"
          ]
        }
      ],
      "source": [
        "import gc, numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.sparse import csr_matrix\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Reduce TF-IDF dimensionality (to lighten memory & noise)\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "sel = VarianceThreshold(threshold=1e-5)\n",
        "X_train_sel = sel.fit_transform(X_train)\n",
        "X_test_sel  = sel.transform(X_test)\n",
        "\n",
        "# Combine numeric features again\n",
        "num_feats_train = train[[\"item_pack_qty\", \"standard_weight_ml\"]].values\n",
        "num_feats_test  = test[[\"item_pack_qty\", \"standard_weight_ml\"]].values\n",
        "X_train_final = csr_matrix(hstack([X_train_sel, num_feats_train]))\n",
        "X_test_final  = csr_matrix(hstack([X_test_sel,  num_feats_test]))\n",
        "\n",
        "y = train[\"price\"].values.astype(float)\n",
        "use_log = True\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": \"mae\",\n",
        "    \"learning_rate\": 0.03,      # smaller learning rate\n",
        "    \"num_leaves\": 96,           # a bit deeper\n",
        "    \"feature_fraction\": 0.75,\n",
        "    \"bagging_fraction\": 0.85,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"lambda_l2\": 1.0,\n",
        "    \"verbosity\": -1,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "NFOLD = 5\n",
        "kf = KFold(n_splits=NFOLD, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(train))\n",
        "test_preds = np.zeros(len(test))\n",
        "\n",
        "print(\"Training tuned LightGBM...\")\n",
        "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_final), 1):\n",
        "    print(f\"\\n---- Fold {fold}/{NFOLD} ----\")\n",
        "    X_tr, X_val = X_train_final[tr_idx], X_train_final[val_idx]\n",
        "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "    if use_log:\n",
        "        y_tr, y_val = np.log1p(y_tr), np.log1p(y_val)\n",
        "    dtr = lgb.Dataset(X_tr, label=y_tr)\n",
        "    dval = lgb.Dataset(X_val, label=y_val)\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtr,\n",
        "        num_boost_round=1500,\n",
        "        valid_sets=[dval],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=50),\n",
        "            lgb.log_evaluation(200)\n",
        "        ]\n",
        "    )\n",
        "    val_pred  = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    test_pred = model.predict(X_test_final, num_iteration=model.best_iteration)\n",
        "    if use_log:\n",
        "        val_pred, test_pred = np.expm1(val_pred), np.expm1(test_pred)\n",
        "    val_pred[val_pred <= 0] = 0.01\n",
        "    test_pred[test_pred <= 0] = 0.01\n",
        "    oof[val_idx] = val_pred\n",
        "    test_preds += test_pred / NFOLD\n",
        "    del model, dtr, dval, X_tr, X_val, y_tr, y_val\n",
        "    gc.collect()\n",
        "\n",
        "smape_val = np.mean(np.abs(oof - y) / ((np.abs(oof) + np.abs(y)) / 2)) * 100\n",
        "print(\"\\nTuned LightGBM SMAPE:\", round(smape_val, 3), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z1lwyX0mUnh",
        "outputId": "bbc56f4b-9264-4a4b-b440-bb412d485bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "OOF SMAPE: 50.4001 %\n",
            "Saved test_out_3.csv with 75000 rows\n",
            "   sample_id      price\n",
            "0     100179  12.584195\n",
            "1     245611  19.401508\n",
            "2     146263  16.220180\n",
            "3      95658  10.581680\n",
            "4      36806  39.879938\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "smape_val = np.mean(np.abs(oof - y) / ((np.abs(oof) + np.abs(y)) / 2)) * 100\n",
        "print(\"\\nOOF SMAPE:\", round(smape_val, 4), \"%\")\n",
        "\n",
        "# Save test predictions\n",
        "submission = pd.DataFrame({\n",
        "    \"sample_id\": test[\"sample_id\"],\n",
        "    \"price\": test_preds\n",
        "})\n",
        "submission.to_csv(\"test_out_3.csv\", index=False)\n",
        "print(f\"Saved test_out_3.csv with {len(submission)} rows\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxsS6AcMf_He"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "def smape_loss(preds, train_data):\n",
        "    labels = train_data.get_label()\n",
        "    # Ensure labels and predictions are non-negative before calculating SMAPE\n",
        "    preds = np.maximum(preds, 0)\n",
        "    labels = np.maximum(labels, 0)\n",
        "    denom = (np.abs(labels) + np.abs(preds)) / 2\n",
        "    # Avoid division by zero for cases where both labels and preds are 0\n",
        "    denom[denom == 0] = 1e-9\n",
        "    grad = np.sign(preds - labels) / denom\n",
        "    hess = 1 / denom\n",
        "    # Handle potential NaNs or Infs\n",
        "    grad[np.isnan(grad)] = 0\n",
        "    hess[np.isnan(hess)] = 0\n",
        "    grad[np.isinf(grad)] = 0\n",
        "    hess[np.isinf(hess)] = 0\n",
        "    return grad, hess\n",
        "\n",
        "# Keep the existing parameters and disable built-in metric\n",
        "params[\"objective\"] = \"regression\"\n",
        "params[\"metric\"] = \"None\"\n",
        "\n",
        "# This cell is now just defining the loss function and updating params.\n",
        "# The training will happen in the previous cell (nSa-wmMdmQk6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH6QBmCihzSb"
      },
      "outputs": [],
      "source": [
        "# After training\n",
        "bias = np.median(y / (oof + 1e-6))\n",
        "oof *= bias\n",
        "test_preds *= bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1hUiO2riERk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression().fit(oof.reshape(-1,1), y)\n",
        "oof = lr.predict(oof.reshape(-1,1))\n",
        "test_preds = lr.predict(test_preds.reshape(-1,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0735fYmiIzs"
      },
      "outputs": [],
      "source": [
        "def extract_brand(text):\n",
        "    text = str(text).split(' ')[0].lower()\n",
        "    text = re.sub(r'[^a-z]', '', text)\n",
        "    return text\n",
        "\n",
        "train['brand'] = train['catalog_content'].apply(extract_brand)\n",
        "test['brand'] = test['catalog_content'].apply(extract_brand)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFj9hknfiLRD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train['brand'] = le.fit_transform(train['brand'])\n",
        "test['brand'] = le.transform(test['brand'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaWZAJ_BiVki"
      },
      "outputs": [],
      "source": [
        "for kw in [\"sauce\",\"shampoo\",\"bottle\",\"can\",\"snack\",\"soap\",\"cream\"]:\n",
        "    train[f\"kw_{kw}\"] = train['catalog_content'].str.contains(kw, case=False, regex=False).astype(int)\n",
        "    test[f\"kw_{kw}\"] = test['catalog_content'].str.contains(kw, case=False, regex=False).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJxPlHPViYIK"
      },
      "outputs": [],
      "source": [
        "train['desc_len'] = train['catalog_content'].str.len()\n",
        "test['desc_len'] = test['catalog_content'].str.len()\n",
        "train['word_count'] = train['catalog_content'].str.split().apply(len)\n",
        "test['word_count'] = test['catalog_content'].str.split().apply(len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7mRjbyLnN1H",
        "outputId": "1a7907aa-1ea1-46c1-b6ae-2249d8c7d3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "OOF SMAPE: 50.4001 %\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nOOF SMAPE:\", round(smape_val, 4), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmUJnn5NrVll",
        "outputId": "fd621950-22c4-400b-e6f2-085cde3ec42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§  Building TF-IDF (20k features)...\n",
            "TF-IDF shapes: (75000, 20000) (75000, 20000)\n",
            "Final shape: (75000, 15281) (75000, 15281)\n",
            "Training DART LightGBM (SMAPE objective)...\n",
            "\n",
            "---- Fold 1/5 ----\n",
            "[200]\tvalid_0's l1: 0.892413\n",
            "[400]\tvalid_0's l1: 0.67469\n",
            "[600]\tvalid_0's l1: 0.605533\n",
            "[800]\tvalid_0's l1: 0.557927\n",
            "[1000]\tvalid_0's l1: 0.547914\n",
            "[1200]\tvalid_0's l1: 0.539444\n",
            "[1400]\tvalid_0's l1: 0.530034\n",
            "[1600]\tvalid_0's l1: 0.523665\n",
            "[1800]\tvalid_0's l1: 0.524322\n",
            "[2000]\tvalid_0's l1: 0.518801\n",
            "[2200]\tvalid_0's l1: 0.517873\n",
            "[2400]\tvalid_0's l1: 0.515487\n",
            "[2600]\tvalid_0's l1: 0.514074\n",
            "[2800]\tvalid_0's l1: 0.513196\n",
            "[3000]\tvalid_0's l1: 0.510344\n",
            "\n",
            "---- Fold 2/5 ----\n",
            "[200]\tvalid_0's l1: 0.896355\n",
            "[400]\tvalid_0's l1: 0.672646\n",
            "[600]\tvalid_0's l1: 0.600237\n",
            "[800]\tvalid_0's l1: 0.550454\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np, re, gc, os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ===== Fill missing text =====\n",
        "train['catalog_content'] = train['catalog_content'].fillna('')\n",
        "test['catalog_content'] = test['catalog_content'].fillna('')\n",
        "\n",
        "# ===== TF-IDF features =====\n",
        "print(\"Building TF-IDF (20k features)...\")\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=5)\n",
        "X_all = pd.concat([train['catalog_content'], test['catalog_content']], axis=0)\n",
        "tfidf.fit(X_all)\n",
        "X_train = tfidf.transform(train['catalog_content'])\n",
        "X_test = tfidf.transform(test['catalog_content'])\n",
        "print(\"TF-IDF shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# ===== Custom numeric features =====\n",
        "def extract_pack_qty(text):\n",
        "    text = str(text).lower()\n",
        "    patterns = [\n",
        "        r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*pcs?', r'(\\d+)\\s*count',\n",
        "        r'x\\s*(\\d+)', r'(\\d+)\\s*pieces?', r'(\\d+)\\s*ct\\b'\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            return float(m.group(1))\n",
        "    return 1.0\n",
        "\n",
        "def extract_standard_weight(text):\n",
        "    text = str(text).lower()\n",
        "    patterns = [\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*kg', 1000.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*g', 1.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*mg', 0.001),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*l', 1000.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*ml', 1.0),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*oz', 28.35),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*pound', 453.6),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*lb', 453.6),\n",
        "        (r'([\\d]*\\.?[\\d]+)\\s*fl', 29.57)\n",
        "    ]\n",
        "    for p, mult in patterns:\n",
        "        m = re.search(p, text)\n",
        "        if m:\n",
        "            try:\n",
        "                val = float(m.group(1))\n",
        "                if val > 0:\n",
        "                    return val * mult\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return 1.0\n",
        "\n",
        "# Apply feature extractors\n",
        "train['item_pack_qty'] = train['catalog_content'].apply(extract_pack_qty)\n",
        "test['item_pack_qty'] = test['catalog_content'].apply(extract_pack_qty)\n",
        "train['standard_weight_ml'] = train['catalog_content'].apply(extract_standard_weight)\n",
        "test['standard_weight_ml'] = test['catalog_content'].apply(extract_standard_weight)\n",
        "\n",
        "# Log-scale numeric features\n",
        "for c in ['item_pack_qty', 'standard_weight_ml']:\n",
        "    train[c] = np.log1p(train[c])\n",
        "    test[c] = np.log1p(test[c])\n",
        "\n",
        "# ===== Feature selection to reduce noise =====\n",
        "sel = VarianceThreshold(threshold=1e-5)\n",
        "X_train_sel = sel.fit_transform(X_train)\n",
        "X_test_sel = sel.transform(X_test)\n",
        "\n",
        "# Combine numeric features\n",
        "num_feats_train = train[['item_pack_qty', 'standard_weight_ml']].values\n",
        "num_feats_test  = test[['item_pack_qty', 'standard_weight_ml']].values\n",
        "X_train_final = csr_matrix(hstack([X_train_sel, num_feats_train]))\n",
        "X_test_final  = csr_matrix(hstack([X_test_sel,  num_feats_test]))\n",
        "\n",
        "print(\"Final shape:\", X_train_final.shape, X_test_final.shape)\n",
        "\n",
        "# ===== LightGBM with DART boosting =====\n",
        "y = train[\"price\"].values.astype(float)\n",
        "NFOLD = 5\n",
        "kf = KFold(n_splits=NFOLD, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(y))\n",
        "test_preds = np.zeros(len(test))\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": \"mae\",  # we'll compute SMAPE manually\n",
        "    \"boosting_type\": \"dart\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"num_leaves\": 64,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 4,\n",
        "    \"lambda_l2\": 4,\n",
        "    \"max_depth\": -1,\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "print(\"Training DART LightGBM (SMAPE objective)...\")\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_final, y)):\n",
        "    print(f\"\\n---- Fold {fold+1}/{NFOLD} ----\")\n",
        "    X_tr, X_val = X_train_final[tr_idx], X_train_final[val_idx]\n",
        "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "    # Log-transform prices\n",
        "    y_tr_log = np.log1p(y_tr)\n",
        "    y_val_log = np.log1p(y_val)\n",
        "\n",
        "    dtr = lgb.Dataset(X_tr, label=y_tr_log)\n",
        "    dval = lgb.Dataset(X_val, label=y_val_log)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtr,\n",
        "        valid_sets=[dval],\n",
        "        num_boost_round=3000,\n",
        "        callbacks=[lgb.log_evaluation(200)]\n",
        "    )\n",
        "\n",
        "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    test_pred = model.predict(X_test_final, num_iteration=model.best_iteration)\n",
        "\n",
        "    # Inverse log transform\n",
        "    val_pred = np.expm1(val_pred)\n",
        "    test_pred = np.expm1(test_pred)\n",
        "\n",
        "    oof[val_idx] = val_pred\n",
        "    test_preds += test_pred / NFOLD\n",
        "\n",
        "\n",
        "    del model, dtr, dval, X_tr, X_val, y_tr, y_val\n",
        "    gc.collect()\n",
        "\n",
        "# ===== SMAPE calculation =====\n",
        "def smape(y_true, y_pred):\n",
        "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "oof_smape = smape(y, oof)\n",
        "print(f\"\\nOOF SMAPE: {oof_smape:.4f}%\")\n",
        "\n",
        "# ===== Save test predictions =====\n",
        "submission = pd.DataFrame({\n",
        "    \"sample_id\": test[\"sample_id\"],\n",
        "    \"price\": test_preds\n",
        "})\n",
        "submission.to_csv(\"test_out_4.csv\", index=False)\n",
        "print(f\"Saved test_out_4.csv with {len(submission)} rows\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KLPOY21WS7lNJyACAF60CLKAJfuGAsWC",
      "authorship_tag": "ABX9TyMDbAKiwpqevcuvqTjYwAAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}